{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Cerulean Hybrid Assembly Workflow\n",
    "by Tessa Rhinehart (contact: rhine3 at gmail dot com)\n",
    "\n",
    "# I. Getting Started\n",
    "\n",
    "This notebook is part-pipeline, part-tutorial. It guides the user through the use of **Cerulean** to make a hybrid assembly on Pink Berry metagenomic data. Cerulean requires the use of **ABySS** to assemble contigs from short paired-end reads, and then the mapping of long reads (e.g. PacBio long reads) to the contigs using **BLASR**. Finally, we fill gaps left by Cerulean using **PBJelly**.\n",
    "\n",
    "\n",
    "### Mounting the data volume\n",
    "\n",
    "Start by creating a snapshot from the Pink Berries Metagenome snapshot, and starting a CeruleanTools AMI Instance with that volume loaded onto it.\n",
    "\n",
    "You have to make a directory for the drive and mount the drive. My metagenome data volume was at `/dev/xvdf`; I'm not sure how to tell where it will be before you load the volume onto the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!lsblk\n",
    "!mkdir ~/data\n",
    "!sudo mount /dev/xvdf ~/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If you make a snapshot in the right zone (e.g. us-east-1d), you can load and mount the volume directly from your instance:\n",
    "\n",
    "```\n",
    "mkdir ~/data\n",
    "\n",
    "aws ec2 attach-volume --volume-id vol-0bdfad3677d717075 --instance-id i-0a62227ff1d1977bd --device /dev/xvdh\n",
    "\n",
    "sudo mount /dev/xvdh ~/data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing binned data\n",
    "\n",
    "Make a new directory where we'll do our work. Then, copy the Illumina short-read files in our new directory. Binning found both alphaproteobacteria (\"a\" prefix) and bacteroidetes (\"b\" prefix)--we only want to copy over the bacteroidetes.\n",
    "\n",
    "We will use the \"illumina_4moleculo\" files; these were prepped for Moleculo sequencing. Illumina was also performed on PacBio-prepped sequences, but the coverage was not as high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd\n",
    "!mkdir hybrid\n",
    "!cp -R ~/data/metagenomes/sequence_reads/illumina_4moleculo/quality-trimmed-reads/reads-by-genome/b* ~/hybrid/reads-by-genome/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Pre-processing\n",
    "\n",
    "### Deinterleaving\n",
    "\n",
    "These are interleaved files. To deinterleave each of them, run the following:\n",
    "```\n",
    "cd ~/hybrid/reads-by-genome/\n",
    "mkdir ~/hybrid/deinterleaved\n",
    "for FILE in *; do \n",
    "mkdir ~/hybrid/deinterleaved/$FILE-deinterleaved;\n",
    "grep -A1 \"_1$\" \"$FILE\" | grep -v \"^--$\" >  ~/hybrid/deinterleaved/$FILE-deinterleaved/reads-1.fasta; \n",
    "grep -A1 \"_2$\" \"$FILE\" | grep -v \"^--$\" >  ~/hybrid/deinterleaved/$FILE-deinterleaved/reads-2.fasta; \n",
    "done\n",
    "```\n",
    "\n",
    "Depending on what the files are named, it might be good to tweak the above to give more sensible names to your folders. I did this manually. However, you probably want to keep the name of each pair of files within each folder the same: reads-1 and reads-2. This makes the assembly step simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming reads \n",
    "\n",
    "ABySS also needs each read to be named with a slash. While in the directory containing the folders for each binned OTU (i.e. ~/hybrid/deinterleaved), the following will replace the\n",
    "\n",
    "    hyphens (e.g. DJB775P1:392:D1R59ACXX:2:1310:17052:38927-2)\n",
    "    with slashes (--> DJB775P1:392:D1R59ACXX:2:1310:17052:38927/2)\n",
    "\n",
    "\n",
    "The -i flag is required to write the results of sed to a file (.bak is necessary for compatibility with certain systems). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd ~/hybrid/deinterleav d/\n",
    "!for FOLDER in * ; do for FILE in $FOLDER/*; do sed -i.bak 's/_2/\\/2/' $FILE; sed -i.bak 's/_1/\\/1/' $FILE; done; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command creates some extraneous .bak files. Delete them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd ~/hybrid/deinterleaved/\n",
    "!for FOLDER in * ; do rm $FOLDER/*.bak; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse-complementing (binned reads only)\n",
    "\n",
    "Due to the binning process, the binned reads are in a forward-forward read format, i.e. both paired-end reads are both from 5' to 3'. (For more conceptual information on this, see http://www.cureffi.org/2012/12/19/forward-and-reverse-reads-in-paired-end-sequencing/.) \n",
    "\n",
    "However, ABySS needs them to be in forward-reverse format, i.e. for each paired read, one needs to be reverse-complemented. We'll use Biopython to reverse-complement the reads.\n",
    "\n",
    "Install Biopython using pip:\n",
    "\n",
    "    pip install biopython\n",
    "\n",
    "Then run the following python script in each binned folder to reverse-complement the `reads-2` files. Any lines that say \"print\" can be commented out if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reverse-complementing the reads in a fasta file, reads-2.fasta\n",
    "#To be run within the folder in which each reads-2 file is located \n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "\n",
    "rc_file = open(\"rc-reads-2.fasta\", \"w+\") #w opens file for writing; \n",
    "                                         #+ creates if it doesn't exist\n",
    "\n",
    "for seq_record in SeqIO.parse(\"reads-2.fasta\", \"fasta\"):\n",
    "    print(\"Reverse-complementing: \"+seq_record.id)\n",
    "    #print(\"ORIGINAL: \" + seq_record.seq)\n",
    "    seqRC = seq_record.reverse_complement(id=True) #preserves seq ID\n",
    "    #print(\"REV-COMP: \" + seqRC.seq)\n",
    "    print(\"Reverse-complementing complete! \") #+ seqRC.id + \"\\n\")\n",
    "    \n",
    "    #write new record to file\n",
    "    rc_file.write(\">\"+str(seqRC.id))\n",
    "    rc_file.write(\"\\n\")\n",
    "    rc_file.write(str(seqRC.seq))\n",
    "    rc_file.write(\"\\n\")\n",
    "\n",
    "rc_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. ABySS: assemble short-read contigs\n",
    "\n",
    "First, we'll use ABySS to assemble the binned bacteroidetes paired-end contigs.\n",
    "\n",
    "Now, assemble the contigs. The flag k=64 is the minimum k-mer length; any sequence shorter than this is discarded. It's probably a good idea to run this in a screen--or a series of screens for each bin. To check on what processes are running, use `top` (and `q` to quit)\n",
    "\n",
    "In the binned case, I preferred doing this individually for each (reverse-complemented) bacteroidetes bin so I could specify a different name for each file, e.g.:\n",
    "\n",
    "```\n",
    "cd ~/hybrid/rev-comp/b1_flavo_deinterleaved\n",
    "abyss-pe name=b1-flavo k=64 in='reads-1.fasta rc-reads-2.fasta'\n",
    "```\n",
    "and so on.\n",
    "\n",
    "```\n",
    "e.g.\n",
    "abyss-pe name=b2-owen k=64 in='reads-1.fasta rc-reads-2.fasta'\n",
    "abyss-pe name=b3-bact k=64 in='reads-1.fasta rc-reads-2.fasta'\n",
    "abyss-pe name=b4-cyt1 k=64 in='reads-1.fasta rc-reads-2.fasta'\n",
    "abyss-pe name=b5-cyt2 k=64 in='reads-1.fasta rc-reads-2.fasta'\n",
    "\n",
    "```\n",
    "\n",
    "For each run this generates 2 files used as inputs to Cerulean:\n",
    "```\n",
    "* <dataname>-contigs.fa    #This contains the contig sequences\n",
    "* <dataname>-contigs.dot   #This contains the graph structure\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. BLASR: map ABySS contigs to long reads\n",
    "\n",
    "The PacBio SmrtTools include BLASR, the program we will use next. The path for the PacBio tools ($SMRT_ROOT) was already set up in the CeruleanTools AMI.\n",
    "\n",
    "### Setting up directories\n",
    "\n",
    "Add a copy of the long reads (rename them to something nice, like pacbio.fasta) and the bacteroidetes *-contigs.fa files to a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir ~/hybrid/blasr\n",
    "!cp ~/hybrid/rev-comp/*/*contigs.fa ~/hybrid/blasr/\n",
    "!cp ~/data/metagenomes/sequence_reads/pacbio/corrected.fasta ~/hybrid/blasr/pacbio.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running BLASR\n",
    "We will use sawriter and BLASR, programs included in the Pacific Biosciences SMRTAnalyis toolkit that is installed on the CeruleanTools AMI.\n",
    "   \n",
    "Our PacBio reads are stored in `pacbio.fasta` and our contigs are stored in `b*-*-contigs.fa`.\n",
    "\n",
    "Note that the `-nproc` flag is set to parallelize BLASR in 31 threads--this is only applicable if your machine has that many cores! \n",
    "\n",
    "```\n",
    " sawriter b1-flavo-contigs.fa\n",
    " blasr pacbio.fasta b1-flavo-contigs.fa -minMatch 10 \\\n",
    "     -minPctIdentity 70 -bestn 30 -nCandidates 30 -maxScore -500 \\\n",
    "     -nproc 31 -noSplitSubreads -header\\\n",
    "     -out b1-flavo_pacbio_contigs_mapping.fasta.m4\n",
    "```\n",
    "\n",
    "blasr pacbio.fasta b3-bact-contigs.fa -minMatch 10 \\\n",
    "     -minPctIdentity 70 -bestn 30 -nCandidates 30 -maxScore -500 \\\n",
    "     -nproc 31 -noSplitSubreads -header\\\n",
    "     -out b3-bact_pacbio_contigs_mapping.fasta.m4\n",
    "\n",
    "#### IMPORTANT NOTE:\n",
    "The -header flag lets us ensure that the fasta.m4 file generated has the following format:\n",
    "```\n",
    "qName tName qStrand tStrand score percentSimilarity tStart tEnd tLength qStart qEnd qLength nCells\n",
    "```\n",
    "\n",
    "However, this header line must be removed from the file before Cerulean is run! Alternatively, you can choose not to include the -header flag. The output of this command is a fasta.m4 file, which we will use in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Cerulean: create assembly from BLASR map\n",
    "\n",
    "Cerulean requires that all input files are in the same directory `<basedir>`. Note that our data are named such that `<dataname>` indicates the name of the bin (i.e. b1-flavo, b2-owen, ..., b5-cyt2). Thus we should have: \n",
    "\n",
    "1. `<basedir>/<dataname>-contigs.fa`\n",
    "2. `<basedir>/<dataname>-contigs.dot`\n",
    "3. `<basedir>/<dataname>_pacbio_contigs_mapping.fasta.m4`\n",
    "\n",
    "Cerulean is run with the following format:\n",
    "```\n",
    "python src/Cerulean.py --dataname <dataname> --basedir <basedir> \\\n",
    " --nproc <numthreads>\n",
    "```\n",
    "e.g.\n",
    "```\n",
    "python ~/Cerulean/src/Cerulean.py --dataname b1-flavo --basedir ~/hybrid/Cerulean \\\n",
    " --nproc 31\n",
    "```\n",
    " \n",
    "This will generate:\n",
    "1.  `<basedir>_cerulean.fasta`\n",
    "2. `<basedir>_cerulean.dot`\n",
    "\n",
    "Note: The .dot file does not have same contigs as fasta, but intermediate graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. PBJelly: fill gaps in Cerulean assembly\n",
    "\n",
    "Cerulean does not include consensus PacBio reads in gaps: the contigs are mapped, but there's just space in the gaps between them. These gaps may be filled using PBJelly. \n",
    "\n",
    "### Creating necessary .qual files\n",
    "Cerulean produces a fasta file. But to use PBJelly, we need either a fastq file or a .qual file. Luckily, PBJelly provides us with fakeQuals.py, which generates a fake .qual file for us to use. (Note: in the CeruleanTools AMI, PBJelly's path is SWEETPATH.) We'll also use fakeQuals.py to make a .qual file for our fasta file, despite the fact that we do have a fastq file. \n",
    "\n",
    "(Would be worth checking to see if we can just use the original fastq file.)\n",
    "\n",
    "Remember that <dataname> must be changed to whatever the filename is.\n",
    "```\n",
    "python $SWEETPATH/bin/fakeQuals.py <dataname>_cerulean.fasta <dataname>_cerulean.qual\n",
    "python $SWEETPATH/bin/fakeQuals.py pacbio.fasta <dataname>_pacbio.qualm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying PBJelly Protocol.xml file:\n",
    "We'll also copy over our PBJelly Protocol.xml file and make a new directory, PBJelly.\n",
    "\n",
    "```\n",
    "cp $SWEETPATH/docs/jellyExample/Protocol.xml <basedir>\n",
    "mkdir PBJelly\n",
    "```\n",
    "\n",
    "Modify Protocol.xml as follows:\n",
    "\n",
    "* Set `<reference>` to `$PATH_TO_<basedir>/<dataname>_cerulean.fasta`\n",
    "* Set `<outputDir>` to `$PATH_TO_<basedir>/PBJelly`\n",
    "* Set `<baseDir>` to `$PATH_TO_<basedir>`\n",
    "* Set `<job>` to `pacbio.fasta`\n",
    "* Set `<blasr>` option `-nproc <numthreads>`\n",
    " \n",
    "(Note: PBJelly requires that the suffix be .fasta and not .fa)\n",
    "\n",
    "### Running PBJelly:\n",
    " \n",
    "Run PBJelly from within `<basedir>` in each of the following stages.\n",
    "\n",
    "```\n",
    "python $SWEETPATH/pbsuite/jelly/Jelly.py <stage> Protocol.xml\n",
    "```\n",
    " \n",
    "where <stage> has to be in the order:\n",
    "\n",
    "```\n",
    "setup\n",
    "mapping\n",
    "support\n",
    "extraction\n",
    "assembly\n",
    "output\n",
    "```\n",
    " \n",
    "The assembled contigs are found in  \n",
    "```\n",
    "<basedir>/PBJelly/jelly.out.fasta\n",
    "```\n",
    " \n",
    "### KNOWN ERROR: \"NO GAPS TO FILL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Wishlist\n",
    "\n",
    "1. Quality testing against other assemblies\n",
    "2. Attempting to use fastq files instead of fasta files --> create fastq file and bypass PBJelly's fakeQuals.py\n",
    "3. Going through similar process with Moleculo long reads instead of PacBio Long reads. Moleculo reads might be higher-quality, which would especially make a difference in gap-filling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
